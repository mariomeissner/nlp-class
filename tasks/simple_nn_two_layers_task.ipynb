{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_nn_two_layers_task.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariomeissner/nlp_class/blob/master/simple_nn_two_layers_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4Zv1v40uphb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCIzSHoOphcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = datasets.load_iris()\n",
        "X, y = data.data, data.target\n",
        "X, y = shuffle(X, y, random_state=1)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrVIIbPwe-T6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize input data\n",
        "X -= X.mean(axis=0)\n",
        "X /= X.max(axis=0)\n",
        "print(X.max(axis=0), X.mean(axis=0).round(decimals=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdcOoZqEVbRA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "yh = np.zeros((y.shape[0], y.max() + 1))\n",
        "yh[range(y.shape[0]), y] = 1\n",
        "y = yh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICEZQAa0phcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Explore the data\n",
        "print(np.concatenate([X, y], axis=1).round(2)[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBdWMzPZphcn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our activation function\n",
        "def sigmoid(x, deriv = False):\n",
        "  ''' The sigmoid of x. \n",
        "  If deriv, then x is the output of a previous sigmoid call. '''\n",
        "  if deriv: return x * (1 - x)\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HD22xyBMphdk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  ''' Softmax over x, assuming we work with rows. '''\n",
        "  e_x = np.exp(x)\n",
        "  return e_x / e_x.sum(axis=-1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Y2Rfyxkphdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy(p, y):\n",
        "  ''' Cross entropy loss between predictions p and true labels y (one-hots). '''\n",
        "  n = y.shape[0]\n",
        "  log_likelihood = -np.log(p)\n",
        "  return np.sum(y * log_likelihood) / n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJJLniEOzE3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(y, pred):\n",
        "  count = 0\n",
        "  for y_r,pred_r in zip(y,pred):\n",
        "    if y_r.argmax() == pred_r.argmax():\n",
        "      count += 1\n",
        "  return count / len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nt68Mmw-I3vn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing softmax\n",
        "print(softmax(np.array([[3,   4,   5],\n",
        "                        [0,  15,  20],\n",
        "                        [0.1,0.2,0.3]])).round(decimals=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGxdXyGHphd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing cross-entropy\n",
        "p1 = np.array([[0.8, 0.1, 0.1], [0.1, 0.7, 0.2]])\n",
        "p2 = np.array([[0.5, 0.3, 0.2], [0.1, 0.6, 0.3]])\n",
        "y1 = np.array([[1,0,0], [0,1,0]])\n",
        "y2 = np.array([[0,0,1], [0,0,1]])\n",
        "# First one should be rather low, second one higher\n",
        "print(cross_entropy(p1, y1))\n",
        "print(cross_entropy(p2, y2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJWkNhQbphcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the data shapes\n",
        "batch, features = X.shape\n",
        "output = 3 # Number of output neurons\n",
        "hidden = 2 # Number of hidden neurons\n",
        "print(f\"Shapes: \\nbatch: {batch}, features: {features}, output: {output}.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCyZdDISg8z2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "# TODO: Give the weights the correct shapes\n",
        "w1 = np.random.normal(size=None) \n",
        "b1 = np.random.normal(size=None)\n",
        "w2 = np.random.normal(size=None)\n",
        "b2 = np.random.normal(size=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cR1SIlXMav84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "lr = 1e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sr1lGvKvpheB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: IMPLEMENT THE FORWARD AND BACKWARD PASS\n",
        "for i in range(10000):\n",
        "\n",
        "  # forward pass\n",
        "  zh    = None\n",
        "  h     = None\n",
        "  zpred = None\n",
        "  pred  = None\n",
        "\n",
        "  # loss\n",
        "  loss = cross_entropy(pred, y)\n",
        "\n",
        "  # Print loss every now and then\n",
        "  if i % 500 == 0:\n",
        "      print(f\"Iteration {i:0>4}, loss: {loss:6.3f}\")\n",
        "\n",
        "  # backpropagation\n",
        "  grad_zpred = pred - y\n",
        "  grad_h     = None\n",
        "  grad_w2    = None\n",
        "  grad_b2    = None\n",
        "  grad_zh    = None\n",
        "  grad_w1    = None\n",
        "  grad_b1    = None\n",
        "\n",
        "  # update weights\n",
        "  w1 -= lr * grad_w1\n",
        "  b1 -= lr * grad_b1 \n",
        "  w2 -= lr * grad_w2     \n",
        "  b2 -= lr * grad_b2 \n",
        "      \n",
        "print(\"Predictions:\")\n",
        "print(np.concatenate((pred, y), axis=1)[:10].round(2))\n",
        "print(f\"Accuracy: {accuracy(y, pred)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}